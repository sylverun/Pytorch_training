{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d804516c",
   "metadata": {},
   "source": [
    "#### Import of the Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddec6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details>\n",
       "<summary>Click to view session information</summary>\n",
       "<pre>\n",
       "-----\n",
       "session_info        1.0.0\n",
       "torch               2.0.0+cu117\n",
       "-----\n",
       "</pre>\n",
       "<details>\n",
       "<summary>Click to view modules imported as dependencies</summary>\n",
       "<pre>\n",
       "asttokens                   NA\n",
       "backcall                    0.2.0\n",
       "colorama                    0.4.6\n",
       "comm                        0.1.2\n",
       "cython_runtime              NA\n",
       "dateutil                    2.8.2\n",
       "debugpy                     1.5.1\n",
       "decorator                   5.1.1\n",
       "entrypoints                 0.4\n",
       "executing                   0.8.3\n",
       "ipykernel                   6.19.2\n",
       "ipython_genutils            0.2.0\n",
       "jedi                        0.18.1\n",
       "mpl_toolkits                NA\n",
       "mpmath                      1.2.1\n",
       "nt                          NA\n",
       "ntsecuritycon               NA\n",
       "numpy                       1.22.3\n",
       "nvfuser                     NA\n",
       "packaging                   23.0\n",
       "parso                       0.8.3\n",
       "pickleshare                 0.7.5\n",
       "pkg_resources               NA\n",
       "platformdirs                2.5.2\n",
       "prompt_toolkit              3.0.36\n",
       "psutil                      5.9.0\n",
       "pure_eval                   0.2.2\n",
       "pydev_ipython               NA\n",
       "pydevconsole                NA\n",
       "pydevd                      2.6.0\n",
       "pydevd_concurrency_analyser NA\n",
       "pydevd_file_utils           NA\n",
       "pydevd_plugins              NA\n",
       "pydevd_tracing              NA\n",
       "pygments                    2.11.2\n",
       "pythoncom                   NA\n",
       "pywintypes                  NA\n",
       "six                         1.16.0\n",
       "stack_data                  0.2.0\n",
       "sympy                       1.11.1\n",
       "tornado                     6.2\n",
       "tqdm                        4.65.0\n",
       "traitlets                   5.7.1\n",
       "typing_extensions           NA\n",
       "wcwidth                     0.2.5\n",
       "win32api                    NA\n",
       "win32com                    NA\n",
       "win32security               NA\n",
       "zmq                         23.2.0\n",
       "</pre>\n",
       "</details> <!-- seems like this ends pre, so might as well be explicit -->\n",
       "<pre>\n",
       "-----\n",
       "IPython             8.10.0\n",
       "jupyter_client      7.4.9\n",
       "jupyter_core        5.2.0\n",
       "notebook            6.5.2\n",
       "-----\n",
       "Python 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)]\n",
       "Windows-10-10.0.22621-SP0\n",
       "-----\n",
       "Session information updated at 2023-05-01 15:51\n",
       "</pre>\n",
       "</details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import session_info\n",
    "session_info.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27035351",
   "metadata": {},
   "source": [
    "##### Creation of a gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aae5ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1143,  0.2841, -0.5264],\n",
      "        [-0.6123,  0.7081, -0.4720],\n",
      "        [ 0.6106,  2.2232, -0.3975]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn((3,3), requires_grad= True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd49acb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1143, 3.2841, 2.4736],\n",
      "        [2.3877, 3.7081, 2.5280],\n",
      "        [3.6106, 5.2232, 2.6025]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y= x+ 3 # Pytorch create the backward propagation automatically (grad_fn) in function of the operation(here :'Add')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a8c7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 38.7956,  43.1403,  24.4749],\n",
      "        [ 22.8051,  54.9995,  25.5625],\n",
      "        [ 52.1460, 109.1273,  27.0919]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z= y*y*4 # (here:'Mul')\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d2a3b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(44.2381, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z=z.mean() # backward propagation take also into account function as mean\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e4b82f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.7683, 2.9192, 2.1988],\n",
      "        [2.1224, 3.2961, 2.2471],\n",
      "        [3.2094, 4.6428, 2.3133]])\n"
     ]
    }
   ],
   "source": [
    "z.backward() # Calcul of the gradient of z in function of x\n",
    "print(x.grad) # x have a gradient attribute where the gradients are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0467befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[52.8347, 77.1650, 44.2443],\n",
      "        [26.7257, 56.7161, 60.0763],\n",
      "        [30.7208, 33.5716, 46.7049]], grad_fn=<MulBackward0>)\n",
      "tensor([[-21.5975, -18.9212,   5.1556],\n",
      "        [ 10.3257,  45.3074,  -5.8241],\n",
      "        [-22.1698, -73.6094, -57.6298]])\n"
     ]
    }
   ],
   "source": [
    "# Note that z must be a scalar in order to perform the backprogation or we need to create a tensor of the same shape than our ouptup and pass it as an argument of the backwardfunction\n",
    "x=torch.randn((3,3), requires_grad= True)\n",
    "y= x+3\n",
    "z= y*y*4\n",
    "print(z)\n",
    "v= torch.randn((3,3)) # if we don't write a similar line, we have an error because z as shape of (3,3) and is not a scalar\n",
    "z.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052b1e0",
   "metadata": {},
   "source": [
    "##### Avoiding gradient calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acb7d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5059, -1.4039, -1.5108],\n",
      "        [ 0.6519,  0.1825,  0.1147],\n",
      "        [ 0.6961,  1.2298, -0.9203]], requires_grad=True)\n",
      "tensor([[0.1327, 0.0459, 0.0281],\n",
      "        [0.8244, 2.1726, 0.0407],\n",
      "        [0.1453, 0.0200, 3.4654]], grad_fn=<MulBackward0>)\n",
      "tensor([[-1.1417, -1.1896, -1.3433],\n",
      "        [ 1.5598,  1.6565,  0.3165],\n",
      "        [ 0.3149,  1.0885, -2.7819]], grad_fn=<AddBackward0>)\n",
      "tensor(-2.1862, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn((3,3), requires_grad= True)\n",
    "y=torch.randn((3,3), requires_grad= True)\n",
    "a= x+y\n",
    "y= y*y\n",
    "z= a*y*4\n",
    "z=z.mean()\n",
    "print(x)\n",
    "print(y)\n",
    "print(a)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "141ce798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6505,  1.1895,  0.0063],\n",
      "        [-0.6821,  0.0202,  1.1396],\n",
      "        [-1.5339, -0.8748,  1.5900]], requires_grad=True)\n",
      "tensor([[3.6010e-02, 5.7549e-03, 1.6263e-01],\n",
      "        [7.4891e-01, 6.9824e-01, 1.2357e-03],\n",
      "        [1.0960e-01, 2.0458e+00, 5.9786e-02]])\n",
      "tensor([[ 1.4607,  1.1136,  0.4095],\n",
      "        [-1.5475,  0.8558,  1.1747],\n",
      "        [-1.8649, -2.3051,  1.8345]], grad_fn=<AddBackward0>)\n",
      "tensor(-2.3311, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# there are three method to block gradient calculation:\n",
    "\n",
    "# y.requires_grad_(False) #modify the variable in place \n",
    "# y.detach() # create a new variable which doesn't requires grad\n",
    "# with torch.no_grad(): # make the calculation without grad for calculation in this statement\n",
    "\n",
    "x=torch.randn((3,3), requires_grad= True)\n",
    "y=torch.randn((3,3), requires_grad= True)\n",
    "a= x+y\n",
    "y.requires_grad_(False)\n",
    "y= y*y\n",
    "z= a*y*4\n",
    "z=z.mean()\n",
    "print(x)\n",
    "print(y) # we see that y have no more grad_fn=<MulBackward0>\n",
    "print(a)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c3482f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8030,  0.4487, -1.1967],\n",
      "        [-0.1160, -0.4889,  0.1093],\n",
      "        [-0.0298,  0.8566,  0.7563]], requires_grad=True)\n",
      "tensor([[-0.6931, -0.8447, -0.1337],\n",
      "        [ 1.3497, -0.4994,  1.0149],\n",
      "        [-0.3147,  0.7554,  0.1300]])\n",
      "tensor([[0.4804, 0.7136, 0.0179],\n",
      "        [1.8218, 0.2494, 1.0301],\n",
      "        [0.0990, 0.5706, 0.0169]])\n",
      "tensor([[ 0.1099, -0.3961, -1.3304],\n",
      "        [ 1.2337, -0.9883,  1.1243],\n",
      "        [-0.3445,  1.6120,  0.8862]], grad_fn=<AddBackward0>)\n",
      "tensor(1.6917, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn((3,3), requires_grad= True)\n",
    "y=torch.randn((3,3), requires_grad= True)\n",
    "a= x+y\n",
    "b= y.detach()\n",
    "y= b*b\n",
    "z= a*y*4\n",
    "z=z.mean()\n",
    "print(x)\n",
    "print(b) # we see that b have no grad_fn and neither y construct from b\n",
    "print(y) \n",
    "print(a)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d88bdc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4044,  0.4430, -0.9974],\n",
      "        [ 1.7554, -0.5372, -2.4439],\n",
      "        [-0.6931,  0.0660,  1.3092]], requires_grad=True)\n",
      "tensor([[0.4598, 2.4854, 2.4870],\n",
      "        [0.8949, 0.8754, 0.7962],\n",
      "        [0.1314, 0.4976, 1.5905]])\n",
      "tensor([[ 1.0825,  2.0195,  0.5796],\n",
      "        [ 0.8094, -1.4729, -3.3362],\n",
      "        [-0.3306, -0.6393,  0.0480]], grad_fn=<AddBackward0>)\n",
      "tensor(1.5341, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn((3,3), requires_grad= True)\n",
    "y=torch.randn((3,3), requires_grad= True)\n",
    "a= x+y\n",
    "with torch.no_grad():\n",
    "    y= y*y\n",
    "z= a*y*4\n",
    "z=z.mean()\n",
    "print(x)\n",
    "print(y) # we see that y have no more grad_fn=<MulBackward0>\n",
    "print(a)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7474e0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9041, -0.5193, -0.5964],\n",
      "        [-2.2481, -0.4334,  0.4614],\n",
      "        [ 0.3522, -0.9819, -1.5909]], requires_grad=True)\n",
      "tensor([[4.0364, 0.2205, 3.3057],\n",
      "        [0.2523, 0.3319, 2.6696],\n",
      "        [1.1538, 1.8679, 4.4042]])\n",
      "tensor([[ 1.1050, -0.0497,  1.2218],\n",
      "        [-2.7504,  0.1427,  2.0953],\n",
      "        [ 1.4263,  0.3848, -3.6895]])\n",
      "tensor(-0.1999)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn((3,3), requires_grad= True)\n",
    "y=torch.randn((3,3), requires_grad= True)\n",
    "with torch.no_grad():\n",
    "    a= x+y\n",
    "    y= y*y\n",
    "z= a*y*4\n",
    "z=z.mean() \n",
    "print(x)\n",
    "print(y) \n",
    "print(a)\n",
    "print(z) # we see that puting 'a' calculation in the with statement remove the grad_fn for z and a  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7601cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0183,  0.6481,  0.1480],\n",
      "        [ 0.8674,  0.9631, -0.9559],\n",
      "        [-0.6429,  0.4659, -0.1904]], requires_grad=True)\n",
      "tensor([[1.5266e-01, 3.2413e-01, 6.0584e-04],\n",
      "        [5.1309e+00, 1.6359e+00, 9.0738e-01],\n",
      "        [4.0881e-01, 1.1446e-02, 1.4528e-01]], grad_fn=<MulBackward0>)\n",
      "tensor([[-0.3725,  0.0788,  0.1234],\n",
      "        [ 3.1325, -0.3159, -1.9085],\n",
      "        [-1.2823,  0.5729,  0.1907]])\n",
      "tensor(5.9123, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x=torch.randn((3,3), requires_grad= True)\n",
    "y=torch.randn((3,3), requires_grad= True)\n",
    "with torch.no_grad():\n",
    "    a= x+y\n",
    "y= y*y\n",
    "z= a*y*4\n",
    "z=z.mean() \n",
    "print(x)\n",
    "print(y) \n",
    "print(a)\n",
    "print(z) # and here the gradient will occur for y but not a and x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76453fe5",
   "metadata": {},
   "source": [
    "##### Avoiding gradient accumulation during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "211d89c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.],\n",
      "        [6., 6., 6.]])\n",
      "tensor([[9., 9., 9.],\n",
      "        [9., 9., 9.],\n",
      "        [9., 9., 9.]])\n"
     ]
    }
   ],
   "source": [
    "# When we call the backward function, the gradients will accumulate into the .grad function\n",
    "\n",
    "x=torch.ones((3,3), requires_grad= True)\n",
    "for epoch in range(3):\n",
    "    output= (x*3).sum()\n",
    "    output.backward()\n",
    "    print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "892e6d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# the gradients become incorrect, so we need to reinitialize the gradient\n",
    "x=torch.ones((3,3), requires_grad= True)\n",
    "for epoch in range(3):\n",
    "    output= (x*3).sum()\n",
    "    output.backward()\n",
    "    print(x.grad)\n",
    "    x.grad.zero_() # this put x.grad to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21aeb692",
   "metadata": {},
   "source": [
    "###### Exemple of backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c946422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x= torch.tensor(1.0)\n",
    "y=torch.tensor(2.0)\n",
    "\n",
    "w= torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass and loss computation\n",
    "y_pred= w*x\n",
    "loss=(y_pred-y)**2\n",
    "print(loss)\n",
    "#backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "# update w\n",
    "# next forward and backward pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30ea43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
